{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ece9b46-bd51-4840-9d17-f66f26c04221",
   "metadata": {},
   "source": [
    "# Work for 2nd Project Update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8758892-3496-4cf1-ba30-c62157e5e39b",
   "metadata": {},
   "source": [
    "First we'll take the relevant methods from preview_request so we can get the information of any song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664f5379-5f28-43f7-a813-3c66d3d78c9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from preview_request import preview_request, convert_to_wav"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7d2699-e8e9-4017-be49-452b239c92a3",
   "metadata": {},
   "source": [
    "We'll need to work with arrays and audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20b9c97-b41b-4266-b410-ae73559fd4f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7a4810-e90f-404e-a0af-1422b02b7406",
   "metadata": {},
   "source": [
    "For some *mild* color manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13914ce-f10e-4edf-838f-f933117a96ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import colorsys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2a89f2-540e-4f82-ae93-96db3fb30b99",
   "metadata": {},
   "source": [
    "For animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee76f20a-88ae-4f9e-8bf8-bfca11e412e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from manim import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbaefc3-f966-49d3-8083-218a19669b6b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Example of Manim in Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6e307f-831d-4d7d-b963-14922e16c418",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RemoveObjectExample(Scene):\n",
    "    def construct(self):\n",
    "        text = Text(\"Alice and Bob\").scale(3)\n",
    "        self.add(text)\n",
    "        self.wait(3)\n",
    "        self.remove(text)\n",
    "        \n",
    "        # Display the circle\n",
    "        circle = Circle()\n",
    "        self.play(Create(circle))\n",
    "        self.wait(1)\n",
    "        self.remove(circle)\n",
    "\n",
    "%manim -ql -v WARNING RemoveObjectExample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0145d58f-70cb-4806-954f-9c0780d174d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# First Method: Changing Backgrounds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab81df6-44e2-4379-92f4-91a90ce4ca69",
   "metadata": {},
   "source": [
    "We'll need to convert from rgb to hex codes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1938a26-bda8-42c5-8493-6667fc522cf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rgb_to_hex(rgb):\n",
    "    '''\n",
    "    param: tuple of rgb coded colors\n",
    "    returns: hex string\n",
    "    '''\n",
    "    if type(rgb) != tuple:\n",
    "        rgb = tuple(rgb)\n",
    "    return '%02x%02x%02x' % rgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0a5f2d-c64c-467f-9e56-1f72b8273860",
   "metadata": {},
   "source": [
    "Normalize values between 0-255 for rgb codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88afcb61-3f18-4222-80af-94fdf658beff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def norm_for_color(array):\n",
    "    '''\n",
    "    param: np array\n",
    "    returns: np array normalized between 0-255\n",
    "    '''\n",
    "    min_val = np.min(array)\n",
    "    max_val = np.max(array)\n",
    "    \n",
    "    normalized_array = (array - min_val) / (max_val - min_val) * 255\n",
    "    return normalized_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe9105f-7ce9-47db-895e-7cd886d11645",
   "metadata": {},
   "source": [
    "Here's the driving manim code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763732f8-e289-4824-9da4-f361bed50ab3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class colorChanger(Scene):\n",
    "    def construct(self):\n",
    "        # audio_file_name = preview_request() #for terminal\n",
    "        audio_file_name = \"GMGM_short.wav\"\n",
    "        y, sr = librosa.load(audio_file_name)\n",
    "\n",
    "        tempo, beat_frames = librosa.beat.beat_track(y=y, sr=sr)\n",
    "        beat_times = librosa.frames_to_time(beat_frames, sr=sr)\n",
    "        \n",
    "        song_time = librosa.get_duration(y=y, sr=sr)\n",
    "        \n",
    "        #Fix last couple seconds\n",
    "        if beat_times[-1] < song_time:\n",
    "            beat_times = np.concatenate((beat_times, np.linspace(beat_times[-1], song_time-0.01, 4)))\n",
    "\n",
    "        percent_through_song = beat_times/song_time\n",
    "\n",
    "\n",
    "        num_to_take_from_y_1 = ((percent_through_song-0.02)*y.size).astype(int)\n",
    "        num_to_take_from_y_2 = ((percent_through_song-0.01)*y.size).astype(int)\n",
    "        num_to_take_from_y_3 = ((percent_through_song+0.00)*y.size).astype(int)\n",
    "\n",
    "        # Normalize the audio values to generate RGB color channels\n",
    "        r = norm_for_color(y[num_to_take_from_y_1]).astype(int)\n",
    "        b = norm_for_color(y[num_to_take_from_y_2]).astype(int)\n",
    "        g = norm_for_color(y[num_to_take_from_y_3]).astype(int)\n",
    "\n",
    "        zip_obj = zip(r,b,g)\n",
    "        color_list = list(zip_obj)\n",
    "\n",
    "        vectorized_function = np.vectorize(rgb_to_hex, signature='(n)->()')\n",
    "        list_of_colors = vectorized_function(np.array(color_list))\n",
    "\n",
    "        cues = np.diff(beat_times)\n",
    "        \n",
    "        self.add_sound(audio_file_name)\n",
    "\n",
    "        for i in range(len(cues)):\n",
    "            text = Text(f\"{beat_times[i]:.2f}\", font_size=144)\n",
    "            self.add(text)  \n",
    "            self.wait(cues[i])\n",
    "\n",
    "            self.camera.background_color = \"#\"+list_of_colors[i]\n",
    "            self.remove(text)\n",
    "            \n",
    "%manim -ql -v WARNING colorChanger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5248a7e4-89e9-4f49-8873-b68c1252fb5e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Second Method:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7dd73d-bc17-4d59-bc4b-0c2c642b3520",
   "metadata": {},
   "source": [
    "We're going to use this song for the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75cd284-431b-411c-80fe-e98376520d65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "audio_file_name = \"GMGM_short.wav\"\n",
    "y, sr = librosa.load(\"GMGM_short.wav\")\n",
    "\n",
    "tempo, beat_frames = librosa.beat.beat_track(y=y, sr=sr)\n",
    "beat_times = librosa.frames_to_time(beat_frames, sr=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ccd016-1203-4185-b7f2-212805d0d8dd",
   "metadata": {},
   "source": [
    "To make the visualizations more \"human\" I'll think about the colors in HSV and since manim will accept hex values, we want a way to convert between the two:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ddad64-974e-4f31-bed9-a5ce148ed777",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hsv_to_hex(hsv):\n",
    "    h, s, v = hsv\n",
    "    #Convert \n",
    "    h = h/360\n",
    "    s = s/100\n",
    "    v = v/100\n",
    "    \n",
    "    # Convert HSV to RGB using colorsys\n",
    "    r, g, b = colorsys.hsv_to_rgb(h, s, v)\n",
    "    \n",
    "    # Convert RGB to HEX\n",
    "    hex_color = '{:02x}{:02x}{:02x}'.format(int(r * 255), int(g * 255), int(b * 255))\n",
    "    \n",
    "    return hex_color"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21c7b18-25cd-455d-a9d1-44a11e96da8b",
   "metadata": {},
   "source": [
    "We'll want a way to normalize a numpy array between 0-n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1594c1ab-1101-443e-8d52-9e9c1adda84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_to_n(array, n=1):\n",
    "    '''\n",
    "    param: np array\n",
    "    returns: np array normalized between 0-n\n",
    "    '''\n",
    "    min_val = np.min(array)\n",
    "    max_val = np.max(array)\n",
    "    \n",
    "    normalized_array = (array - min_val) / (max_val - min_val) * n\n",
    "    return normalized_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bc8b53-d337-45eb-8ca0-acbef120237b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 1: Get *Pitch* for Brightness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466930f7-db69-455f-978e-25e02d1e7a74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to extract pitch at specific timestamps\n",
    "def extract_max_pitches(y, sr, beat_times, duration=0.1):\n",
    "    pitches = []\n",
    "    for time in beat_times:\n",
    "        # Extract a segment of audio around each timestamp\n",
    "        start_sample = int(time * sr)\n",
    "        end_sample = int((time + duration) * sr)\n",
    "        segment = y[start_sample:end_sample]\n",
    "\n",
    "        # Get pitch\n",
    "        pitches_segment, magnitudes = librosa.core.piptrack(y=segment, sr=sr)\n",
    "        \n",
    "        # Get the maximum pitch from the frame with the highest magnitude\n",
    "        max_pitch = np.max(pitches_segment)\n",
    "        if max_pitch > 0:\n",
    "            pitches.append(max_pitch)\n",
    "        else:\n",
    "            pitches.append(0)  # No pitch detected\n",
    "    \n",
    "    return np.array(pitches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb404c5-455c-4eb3-bb41-74126b0b5858",
   "metadata": {},
   "source": [
    "Let's test out the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576c74ad-0336-4e21-b76b-c7f4ec1b8b9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pitches = extract_max_pitches(y,sr,beat_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806f1bf8-7717-4fb8-a824-6ce536323b6a",
   "metadata": {},
   "source": [
    "Notice `beat_times` and `pitches` have the same size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00e5b72-371f-427d-a2a1-9be124f0e9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "beat_times.shape, pitches.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ff7654-ebc3-43d4-8798-48223f429cf3",
   "metadata": {},
   "source": [
    "Here's what the `pitches` array looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c483f3-84c9-4e39-842e-7f2bc5544c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pitches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca88b9b-0e05-4533-9603-e258882c4acc",
   "metadata": {
    "tags": []
   },
   "source": [
    "And what normalizing will do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b37fa1-c3fc-422e-ba8b-0be4b9642564",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = norm_to_n(pitches, 100)\n",
    "B.astype(int) #for viewing purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0f08d5-5f00-4e8f-a5c9-1e78af0a3834",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 2: Get Volume for Saturation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594d4676-ea41-4190-afcc-b003c054ed6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract pitch at specific timestamps\n",
    "def extract_max_volume(y, sr, beat_times, duration=0.1):\n",
    "    volumes = []\n",
    "    for time in beat_times:\n",
    "        # Extract a segment of audio around each timestamp\n",
    "        start_sample = int(time * sr)\n",
    "        end_sample = int((time + duration) * sr)\n",
    "        segment = y[start_sample:end_sample]\n",
    "\n",
    "        # Get volume\n",
    "        rms = librosa.feature.rms(y=segment)\n",
    "        rms_db = librosa.amplitude_to_db(rms)\n",
    "        \n",
    "        # Get the maximum volume\n",
    "        max_vol = np.max(rms_db)\n",
    "        volumes.append(max_vol)\n",
    "    \n",
    "    return np.array(volumes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a17b955-bf19-4682-a7e7-b37bc36ad069",
   "metadata": {},
   "outputs": [],
   "source": [
    "volumes = extract_max_volume(y, sr, beat_times)\n",
    "volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b682988-7ab7-477b-988c-7730c8a5cd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "S = norm_to_n(volumes, 100)\n",
    "S.astype(int) #for viewing purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc5767f-a105-43a9-8c66-d5f61fcd2633",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 3: Get Note (within Octave) for Hue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703cd5ba-4c74-4f47-8442-cc3b52bb692a",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = librosa.hz_to_note(pitches, octave=False)\n",
    "notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581f85dd-83e6-4a26-9680-2d59d197f2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "note_map = {\n",
    "    'C':1,\n",
    "    'C♯':1.5,\n",
    "    'D':2,\n",
    "    'D♯':2.5,\n",
    "    'E':3,\n",
    "    'F':4,\n",
    "    'F♯':4.5,\n",
    "    'G':5,\n",
    "    'G♯':5.5,\n",
    "    'A':6,\n",
    "    'A♯':5.5,\n",
    "    'B':7\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86acfe03-59e5-477a-9b6f-f3cab3a459f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "note_vals = [note_map[note] for note in notes]\n",
    "np.array(note_vals) #made nparray for viewing purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20332dc-a567-42a1-a228-3e768783b616",
   "metadata": {},
   "outputs": [],
   "source": [
    "H = norm_to_n(note_vals,360)\n",
    "H.astype(int)  #for viewing purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27777b4-90eb-4b0a-a17f-4b5fd8b52218",
   "metadata": {},
   "source": [
    "## Step 4: Separate and Detect Different Sounds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cac67f-0a7d-42d2-beb9-382de70a1d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#harmonic percussive separation \n",
    "y_harmonic, y_percussive = librosa.effects.hpss(y)\n",
    "y_harmonic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c190e99-0e96-4ba7-899b-44149deca431",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_percussive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f3ebb6-9ecd-429d-9f39-a8b7909d8b63",
   "metadata": {},
   "source": [
    "We can detect local note onset events with librosa in our percussive array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9d2bdf-2c63-4691-9e80-cac51d8dd768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# onset detection\n",
    "o_env = librosa.onset.onset_strength(y=y_percussive, sr=sr)\n",
    "\n",
    "\n",
    "# onset strength per unit\n",
    "onset_frames = librosa.onset.onset_detect(y=y_percussive, sr=sr, units=\"time\")\n",
    "onset_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994bb199-3359-463d-a449-6552be4db06e",
   "metadata": {},
   "source": [
    "From the documentation, here is an example of how we can visualize onsets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0624f70c-7f0d-4555-aaca-b4c7af582161",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "times = librosa.times_like(o_env, sr=sr)\n",
    "\n",
    "D = np.abs(librosa.stft(y))\n",
    "fig, ax = plt.subplots(nrows=2, sharex=True)\n",
    "librosa.display.specshow(librosa.amplitude_to_db(D, ref=np.max),x_axis='time', y_axis='log', ax=ax[0], sr=sr)\n",
    "\n",
    "ax[1].plot(times, o_env, label='Onset strength')\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb434f5-38c4-4d91-b727-eaef53ea2063",
   "metadata": {},
   "source": [
    "We can also detect the spectral centroids per unit to maybe do something like exapnd the text during high frequency, brighter sounds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ca0e4b-3e76-4021-a992-473180a04d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)[0]\n",
    "norm_centroid = (spectral_centroid - spectral_centroid.min()) / (spectral_centroid.max() - spectral_centroid.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03472ca1-9e07-4a15-b932-3430cdd652fa",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Zip and Colorize!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3441dfbb-1d95-4060-a65f-52845facaed1",
   "metadata": {},
   "source": [
    "As before, we'll zip to make the color channels and colorize!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29386ccb-1ce6-40ec-9406-8886315f7903",
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_obj = zip(H,S,B)\n",
    "color_list = list(zip_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30b8ca7-e3a2-480f-b867-bad6ecbfa3f8",
   "metadata": {},
   "source": [
    "Let's look at the first ~10 values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac07f419-2531-465c-98ca-709fd3c5fd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_list[:10] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5485d892-b288-4550-b6ae-ff9b37cad293",
   "metadata": {},
   "source": [
    "Again we'll vectorize the function and apply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e294aa9-6557-4980-b722-a885e98306c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_function = np.vectorize(hsv_to_hex, signature='(n)->()')\n",
    "list_of_colors = vectorized_function(np.array(color_list))\n",
    "list_of_colors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7e9c35-6dc7-429a-a9fc-f68454c1d131",
   "metadata": {},
   "source": [
    "## Play!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a0518b-ddcb-48ce-be91-9adbe6e7937c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class hsvAnim(Scene):\n",
    "    def construct(self):\n",
    "        # audio_file_name = preview_request()\n",
    "        # y, sr = librosa.load(audio_file_name)\n",
    "        audio_file_name = \"GMGM_30.wav\"\n",
    "        y, sr = librosa.load(\"GMGM_30.wav\")\n",
    "        \n",
    "        tempo, beat_frames = librosa.beat.beat_track(y=y, sr=sr)\n",
    "        beat_times = librosa.frames_to_time(beat_frames, sr=sr)\n",
    "\n",
    "        song_time = librosa.get_duration(y=y, sr=sr)\n",
    "\n",
    "        #prevent last 5 sec from being constant\n",
    "        if beat_times[-1] < song_time:\n",
    "            beat_times = np.concatenate((beat_times, np.linspace(beat_times[-1], song_time-0.01, 4)))\n",
    "\n",
    "        # harmonic-percussive separation\n",
    "        y_harmonic, y_percussive = librosa.effects.hpss(y)\n",
    "\n",
    "        # onset detection \n",
    "        # onset_env = librosa.onset.onset_strength(y=y_percussive, sr=sr)\n",
    "        # onset_times = librosa.onset.onset_detect(y=y_percussive, sr=sr, units=\"time\")\n",
    "\n",
    "        # spectral centroid for brightness \n",
    "        spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)[0]\n",
    "        norm_centroid = (spectral_centroid - spectral_centroid.min()) / (spectral_centroid.max() - spectral_centroid.min())\n",
    "\n",
    "        # Extract Pitch and Volume\n",
    "        pitches = extract_max_pitches(y, sr, beat_times)\n",
    "        volumes = extract_max_volume(y, sr, beat_times)\n",
    "        \n",
    "        note_map = {\n",
    "            'C':1,'C♯':1.5,\n",
    "            'D':2,'D♯':2.5,\n",
    "            'E':3,\n",
    "            'F':4,'F♯':4.5,\n",
    "            'G':5,'G♯':5.5,\n",
    "            'A':6,'A♯':5.5,\n",
    "            'B':7\n",
    "        }\n",
    "        \n",
    "        pitches = extract_max_pitches(y, sr, beat_times)\n",
    "        volumes = extract_max_volume(y, sr, beat_times)\n",
    "\n",
    "        notes = librosa.hz_to_note(pitches, octave=False)\n",
    "        note_vals = [note_map[note] for note in notes]\n",
    "\n",
    "        H = norm_to_n(note_vals,360)\n",
    "        S = norm_to_n(volumes, 100)\n",
    "        B = norm_to_n(pitches, 100)\n",
    "\n",
    "\n",
    "        zip_obj = zip(H,S,B)\n",
    "        color_list = list(zip_obj)\n",
    "\n",
    "        vectorized_function = np.vectorize(hsv_to_hex, signature='(n)->()')\n",
    "        list_of_colors = vectorized_function(np.array(color_list))\n",
    "\n",
    "        cues = np.diff(beat_times)\n",
    "        \n",
    "        self.add_sound(audio_file_name)\n",
    "\n",
    "\n",
    "        for i in range(len(cues)):\n",
    "            text = Text(f\"{beat_times[i]:.2f}\", font_size=144)\n",
    "            # text size expands during high frequency sounds \n",
    "            text.scale(1 + norm_centroid[i] * 6)\n",
    "            \n",
    "            self.add(text)  \n",
    "            self.wait(cues[i])\n",
    "            self.camera.background_color = \"#\"+list_of_colors[i]\n",
    "            self.remove(text)\n",
    "            \n",
    "        self.wait(3)\n",
    "    \n",
    "%manim -ql -v WARNING hsvAnim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46689872-70c0-416b-9950-76807b97c1ab",
   "metadata": {},
   "source": [
    "# Some Extra Animations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a740b39-a7a6-4017-9094-489dbd21876c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class RosePattern(VMobject):\n",
    "    def __init__(self, radius: float = 2, k: float = 3, theta_range=TAU, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.radius = radius\n",
    "        self.k = k\n",
    "\n",
    "        step_size = 0.1\n",
    "        theta = np.arange(0, theta_range + step_size, step_size)\n",
    "\n",
    "        points = [\n",
    "            [\n",
    "                radius * np.cos(k * t) * np.cos(t),\n",
    "                radius * np.cos(k * t) * np.sin(t),\n",
    "                0\n",
    "            ] for t in theta\n",
    "        ]\n",
    "\n",
    "        self.set_points_smoothly(points)\n",
    "        \n",
    "class ShowingRosePattern(Scene):\n",
    "    def construct(self):\n",
    "        self.camera.background_color = '#7fbf26'\n",
    "        rose = RosePattern(k=8, color=RED, stroke_width=15)\n",
    "\n",
    "        self.play(Create(rose), run_time=5)\n",
    "        self.wait()\n",
    "        \n",
    "%manim -ql -v WARNING ShowingRosePattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370feb5c-bb0d-4105-a10d-436f24495758",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c9807cfb-1c2b-4131-91c0-15bfee3ecf30",
   "metadata": {},
   "source": [
    "# Other Project updates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679b572b-44b7-4bc9-8621-0b2fba43e0c7",
   "metadata": {},
   "source": [
    "Example of using onset detection for color change instead of beats:\n",
    "- issues with matching up onset times with color list and cues but first 5 seconds works (with little visual appeal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ee85df-724b-4161-883e-6a18091d2240",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class hsvAnim(Scene):\n",
    "    def construct(self):\n",
    "        # audio_file_name = preview_request()\n",
    "        # y, sr = librosa.load(audio_file_name)\n",
    "        audio_file_name = \"GMGM_30.wav\"\n",
    "        y, sr = librosa.load(\"GMGM_30.wav\")\n",
    "        \n",
    "        tempo, beat_frames = librosa.beat.beat_track(y=y, sr=sr)\n",
    "        # beat_times = librosa.frames_to_time(beat_frames, sr=sr)\n",
    "\n",
    "        song_time = librosa.get_duration(y=y, sr=sr)\n",
    "\n",
    "        #prevent last 5 sec from being constant\n",
    "        # if beat_times[-1] < song_time:\n",
    "        #     beat_times = np.concatenate((beat_times, np.linspace(beat_times[-1], song_time-0.01, 4)))\n",
    "\n",
    "        # harmonic-percussive separation\n",
    "        y_harmonic, y_percussive = librosa.effects.hpss(y)\n",
    "\n",
    "        # onset detection \n",
    "        onset_env = librosa.onset.onset_strength(y=y_percussive, sr=sr)\n",
    "        onset_times = librosa.onset.onset_detect(y=y_percussive, sr=sr, units=\"time\")\n",
    "\n",
    "        # spectral centroid for brightness \n",
    "        spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)[0]\n",
    "        norm_centroid = (spectral_centroid - spectral_centroid.min()) / (spectral_centroid.max() - spectral_centroid.min())\n",
    "\n",
    "        # Extract Pitch and Volume\n",
    "        pitches = extract_max_pitches(y, sr, beat_times)\n",
    "        volumes = extract_max_volume(y, sr, beat_times)\n",
    "        \n",
    "        note_map = {\n",
    "            'C':1,'C♯':1.5,\n",
    "            'D':2,'D♯':2.5,\n",
    "            'E':3,\n",
    "            'F':4,'F♯':4.5,\n",
    "            'G':5,'G♯':5.5,\n",
    "            'A':6,'A♯':5.5,\n",
    "            'B':7\n",
    "        }\n",
    "\n",
    "\n",
    "        pitches = extract_max_pitches(y, sr, beat_times)\n",
    "        volumes = extract_max_volume(y, sr, beat_times)\n",
    "\n",
    "        notes = librosa.hz_to_note(pitches, octave=False)\n",
    "        note_vals = [note_map[note] for note in notes]\n",
    "\n",
    "        H = norm_to_n(note_vals,360)\n",
    "        S = norm_to_n(volumes, 100)\n",
    "        B = norm_to_n(pitches, 100)\n",
    "\n",
    "\n",
    "        zip_obj = zip(H,S,B)\n",
    "        color_list = list(zip_obj)\n",
    "\n",
    "        vectorized_function = np.vectorize(hsv_to_hex, signature='(n)->()')\n",
    "        list_of_colors = vectorized_function(np.array(color_list))\n",
    "    \n",
    "        self.add_sound(audio_file_name)\n",
    "\n",
    "        # trying to add color change based on onsets\n",
    "        num_onsets = len(onset_times)\n",
    "        num_colors = len(list_of_colors)\n",
    "\n",
    "        if num_onsets > num_colors:\n",
    "            onset_times = onset_times[:num_colors]  # Clip onset_times to match number of colors\n",
    "        elif num_onsets < num_colors:\n",
    "            list_of_colors = list_of_colors[:num_onsets]  # Clip list_of_colors to match onset_times\n",
    "\n",
    "        cues = np.diff(onset_times)\n",
    "        \n",
    "        for i in range(min(len(cues), len(list_of_colors))):\n",
    "            text = Text(f\"{onset_times[i]:.2f}\", font_size=144)\n",
    "            text.scale(1 + norm_centroid[i] * 6)\n",
    "\n",
    "            # wait_time = cues[i] if cues[i] > 0.1 else 0.1 \n",
    "            \n",
    "            self.add(text)\n",
    "            self.wait(cues[i]) \n",
    "            self.camera.background_color = \"#\"+list_of_colors[i]  \n",
    "            self.remove(text)  \n",
    "\n",
    "        # for i in range(len(cues)):\n",
    "        #     text = Text(f\"{beat_times[i]:.2f}\", font_size=144)\n",
    "        #     # text size expands during high frequency sounds \n",
    "        #     text.scale(1 + norm_centroid[i] * 6)\n",
    "            \n",
    "\n",
    "        #     self.add(text)  \n",
    "        #     self.wait(cues[i])\n",
    "        #     self.camera.background_color = \"#\"+list_of_colors[i]\n",
    "        #     self.remove(text)\n",
    "\n",
    "        duration = sum(cues) + 3\n",
    "        self.wait(duration)\n",
    "    \n",
    "%manim -ql -v WARNING hsvAnim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b971cb0-0653-4bfe-9475-a833979daf9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
